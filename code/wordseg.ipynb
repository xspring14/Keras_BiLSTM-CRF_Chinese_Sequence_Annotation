{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b05eebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e7d0bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow_addons.text import crf_log_likelihood, viterbi_decode, crf_decode\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Bidirectional, Masking, Dense, Layer, TimeDistributed, Activation, Lambda, Dropout, InputSpec\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.layers import Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2d0e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e300676",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4589f339",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_42948/337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 11:02:21.752218: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-14 11:02:21.828420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-14 11:02:21.890527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-14 11:02:21.890780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-14 11:02:22.693986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-14 11:02:22.694216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-14 11:02:22.694416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-14 11:02:22.694590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 2257 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c6d990",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b539113a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  “  人们  常  说  生活  是  一  部  教科书  ，  而  血  与  火  ...\n",
      "1  “  心  静  渐  知  春  似  海  ，  花  深  每  觉  影  生  香  。\n",
      "2    “  吃  屎  的  东西  ，  连  一  捆  麦  也  铡  不  动  呀  ？\n",
      "3  他  “  严格要求  自己  ，  从  一个  科举  出身  的  进士  成为  一...\n",
      "4  “  征  而  未  用  的  耕地  和  有  收益  的  土地  ，  不准  ...\n",
      "                                                   0\n",
      "0                      扬帆  远东  做  与  中国  合作  的  先行  \n",
      "1                            希腊  的  经济  结构  较  特殊  。\n",
      "2  海运  业  雄踞  全球  之  首  ，  按  吨位  计  占  世界  总数  的...\n",
      "3  另外  旅游  、  侨汇  也是  经济  收入  的  重要  组成部分  ，  制造业...\n",
      "4  多年来  ，  中  希  贸易  始终  处于  较低  的  水平  ，  希腊  几乎...\n"
     ]
    }
   ],
   "source": [
    "train_set = pd.read_csv('data/msr_training.utf8', encoding= 'utf8', header=None)  # 不把第一行作为列属性，且pd读出来就是数据帧，就是字符串\n",
    "test_set = pd.read_csv('data/msr_test_gold.utf8', encoding='utf8', header=None)\n",
    "print(train_set.head())\n",
    "print(test_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "144bc168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将句子转换成字序列\n",
    "def get_char(sentence):\n",
    "    char_list = []\n",
    "    sentence = ''.join(sentence.split('  ')) #去掉空格\n",
    "    for i in sentence:\n",
    "        char_list.append(i)\n",
    "    return char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2afb897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将句子转成BMES序列\n",
    "def get_label(sentence):\n",
    "    result = []\n",
    "    word_list = sentence.split('  ')  #两个空格来分隔一个词\n",
    "    for i in range(len(word_list)):\n",
    "        if len(word_list[i]) == 1:\n",
    "            result.append('S')\n",
    "        elif len(word_list[i]) == 2:\n",
    "            result.append('B')\n",
    "            result.append('E')\n",
    "        else:\n",
    "            temp = len(word_list[i]) - 2\n",
    "            result.append('B')\n",
    "            result.extend('M'*temp)\n",
    "            result.append('E')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a5d6731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    char, content, label = [], [], []\n",
    "    maxlen = 0\n",
    "\n",
    "    for i in range(len(file)):  # 记得加range！！\n",
    "        line = file.loc[i,0]   # 用loc来访问dataframe\n",
    "        line = line.strip('\\n') #去掉换行符\n",
    "        line = line.strip(' ')  #去掉开头和结尾的空格\n",
    "        \n",
    "        char_list = get_char(line)        #获得字列表\n",
    "        label_list = get_label(line)      # 获得标签列表\n",
    "        maxlen = max(maxlen, len(char_list))\n",
    "        if len(char_list)!=len(label_list):\n",
    "            continue   # 由于数据集的问题，所以要删掉有问题的样本（在训练集中有26个样本；测试集中无）\n",
    "        char.extend(char_list)            #每一个单元是1个字\n",
    "        content.append(char_list)         # 每一个单元是一行里面的各个字（分好）\n",
    "        label.append(label_list)          #每一个单元是一行里面打好标签的结果（含标点）\n",
    "    return char, content, label, maxlen  #word是单列表，content和label是双层列表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1834d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data: padding\n",
    "def process_data(char_list, label_list, vocab, chunk_tags, MAXLEN):\n",
    "    # idx2vocab = {idx: char for idx, char in enumerate(vocab)}\n",
    "    vocab2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "    # get every char of every word, map to idx in vocab, set to <UNK> if not in vocab\n",
    "    x = [[vocab2idx.get(char, 1) for char in s] for s in char_list]\n",
    "    # map label to idx\n",
    "    y_chunk = [[chunk_tags.index(label) for label in s] for s in label_list]\n",
    "    # padding of x, default is 0(symbolizes <PAD>). padding includes:over->cutoff, less->padding. default: left_padding\n",
    "    x = pad_sequences(x, maxlen=MAXLEN, value=0)\n",
    "    # padding of y_chunk\n",
    "    y_chunk = pad_sequences(y_chunk, maxlen=MAXLEN, value=-1)\n",
    "    # one_hot:\n",
    "    y_chunk = to_categorical(y_chunk, len(chunk_tags))\n",
    "    # y_chunk = np.eye(len(chunk_tags), dtype='float32')[y_chunk]\n",
    "    return x, y_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c6e11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    chunk_tags = ['S','B','M','E']\n",
    "    train_char, train_content, train_label, _ = read_file(train_set)\n",
    "    test_char, test_content, test_label, maxlen = read_file(test_set)\n",
    "    \n",
    "    vocab = list(set(train_char + test_char))   # 合并，构成大词表\n",
    "    special_chars = ['<PAD>', '<UNK>']   #特殊词表示：PAD表示padding，UNK表示词表中没有\n",
    "    vocab = special_chars + vocab\n",
    "    \n",
    "    # save initial config data\n",
    "#     with open(SAVE_PATH, 'wb') as f:\n",
    "#         pickle.dump((train_char, chunk_tags), f)\n",
    "    \n",
    "    # process data: padding\n",
    "    print('maxlen is %d' % maxlen)\n",
    "    train_x, train_y = process_data(train_content, train_label, vocab, chunk_tags, maxlen)\n",
    "    test_x, test_y = process_data(test_content, test_label, vocab, chunk_tags, maxlen)\n",
    "    return train_x, train_y, test_x, test_y, vocab, chunk_tags, maxlen, test_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4805e5b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen is 308\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y, vocab, chunk_tags, max_len, test_content = load_data()\n",
    "\n",
    "n_words = len(vocab)\n",
    "n_tags = len(chunk_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd55917",
   "metadata": {},
   "source": [
    "## 构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6a6cfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeddings_matrix(word2vec_model, vocab):\n",
    "    char2vec_dict = {}    # 字对词向量\n",
    "    vocab2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "    for char, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\n",
    "        char2vec_dict[char] = vector\n",
    "    embeddings_matrix = np.zeros((len(vocab), EMBED_DIM))# form huge matrix\n",
    "    for i in tqdm(range(2, len(vocab))):\n",
    "        char = vocab[i]\n",
    "        if char in char2vec_dict.keys():    # 如果char在词向量列表中，更新权重；否则，赋值为全0（默认）\n",
    "            char_vector = char2vec_dict[char]\n",
    "            embeddings_matrix[i] = char_vector\n",
    "    return embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45dc499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSelf(Layer):\n",
    "    \"\"\"\n",
    "        self attention,\n",
    "        codes from:  https://mp.weixin.qq.com/s/qmJnyFMkXVjYBwoR_AQLVA\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # W、K and V\n",
    "        self.kernel = self.add_weight(name='WKV',\n",
    "                                        shape=(3, input_shape[2], self.output_dim),\n",
    "                                        initializer='uniform',\n",
    "                                        regularizer=L1L2(0.0000032),\n",
    "                                        trainable=True)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        WQ = K.dot(x, self.kernel[0])\n",
    "        WK = K.dot(x, self.kernel[1])\n",
    "        WV = K.dot(x, self.kernel[2])\n",
    "        print(\"WQ.shape\",WQ.shape)\n",
    "        print(\"K.permute_dimensions(WK, [0, 2, 1]).shape\",K.permute_dimensions(WK, [0, 2, 1]).shape)\n",
    "        QK = K.batch_dot(WQ,K.permute_dimensions(WK, [0, 2, 1]))\n",
    "        QK = QK / (100**0.5)\n",
    "        QK = K.softmax(QK)\n",
    "        print(\"QK.shape\",QK.shape)\n",
    "        V = K.batch_dot(QK,WV)\n",
    "        return V\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],input_shape[1],self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d8f64beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layer(input_dim, output_dim, input_length, mask_zero):\n",
    "    return Embedding(input_dim = input_dim, output_dim = output_dim, input_length = input_length, mask_zero = mask_zero)\n",
    "    \n",
    "def bilstm_crf(maxlen, n_tags, embedding_dim, n_words, mask_zero, training = True):\n",
    "    \"\"\"\n",
    "    bilstm_crf - module to build BiLSTM-CRF model\n",
    "    Inputs:\n",
    "        - input_shape : tuple\n",
    "            Tensor shape of inputs, excluding batch size\n",
    "    Outputs:\n",
    "        - output : tensorflow.keras.outputs.output\n",
    "            BiLSTM-CRF output\n",
    "    \"\"\"\n",
    "    input = Input(shape = (maxlen,))\n",
    "    \n",
    "    # Embedding layer\n",
    "    embeddings = embedding_layer(input_dim=n_words, output_dim=embedding_dim, input_length=maxlen, mask_zero=mask_zero)\n",
    "    output = embeddings(input)\n",
    "\n",
    "    # BiLSTM layer\n",
    "    output = Bidirectional(LSTM(units=50, return_sequences=True, recurrent_activation = 'sigmoid', recurrent_dropout=0, activation='tanh'))(output)\n",
    "#     output = AttentionSelf(200)(output)\n",
    "\n",
    "    # Dense layer\n",
    "    output = TimeDistributed(Dense(n_tags, activation='relu'))(output)\n",
    "    \n",
    "    output = CRF(n_tags, name='crf_layer')(output)\n",
    "    return Model(input, output)\n",
    "\n",
    "\n",
    "class CRF(Layer):\n",
    "    def __init__(self,\n",
    "                 output_dim,\n",
    "                 sparse_target=True,\n",
    "                 **kwargs):\n",
    "        \"\"\"    \n",
    "        Args:\n",
    "            output_dim (int): the number of labels to tag each temporal input.\n",
    "            sparse_target (bool): whether the the ground-truth label represented in one-hot.\n",
    "        Input shape:\n",
    "            (batch_size, sentence length, output_dim)\n",
    "        Output shape:\n",
    "            (batch_size, sentence length, output_dim)\n",
    "        \"\"\"\n",
    "        super(CRF, self).__init__(**kwargs)\n",
    "        self.output_dim = int(output_dim) \n",
    "        self.sparse_target = sparse_target\n",
    "        self.input_spec = InputSpec(min_ndim=3)\n",
    "        self.supports_masking = False\n",
    "        self.sequence_lengths = None\n",
    "        self.transitions = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        f_shape = tf.TensorShape(input_shape)\n",
    "        input_spec = InputSpec(min_ndim=3, axes={-1: f_shape[-1]})\n",
    "\n",
    "        if f_shape[-1] is None:\n",
    "            raise ValueError('The last dimension of the inputs to `CRF` '\n",
    "                             'should be defined. Found `None`.')\n",
    "        if f_shape[-1] != self.output_dim:\n",
    "            raise ValueError('The last dimension of the input shape must be equal to output'\n",
    "                             ' shape. Use a linear layer if needed.')\n",
    "        self.input_spec = input_spec\n",
    "        self.transitions = self.add_weight(name='transitions',\n",
    "                                           shape=[self.output_dim, self.output_dim],\n",
    "                                           initializer='glorot_uniform',\n",
    "                                           trainable=True)\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # Just pass the received mask from previous layer, to the next layer or\n",
    "        # manipulate it if this layer changes the shape of the input\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, sequence_lengths=None, training=None, **kwargs):\n",
    "        sequences = tf.convert_to_tensor(inputs, dtype=self.dtype)\n",
    "        if sequence_lengths is not None:\n",
    "            assert len(sequence_lengths.shape) == 2\n",
    "            assert tf.convert_to_tensor(sequence_lengths).dtype == 'int32'\n",
    "            seq_len_shape = tf.convert_to_tensor(sequence_lengths).get_shape().as_list()\n",
    "            assert seq_len_shape[1] == 1\n",
    "            self.sequence_lengths = K.flatten(sequence_lengths)\n",
    "        else:\n",
    "            self.sequence_lengths = tf.ones(tf.shape(inputs)[0], dtype=tf.int32) * (\n",
    "                tf.shape(inputs)[1]\n",
    "            )\n",
    "\n",
    "        viterbi_sequence, _ = crf_decode(sequences,\n",
    "                                         self.transitions,\n",
    "                                         self.sequence_lengths)\n",
    "        output = K.one_hot(viterbi_sequence, self.output_dim)\n",
    "        return K.in_train_phase(sequences, output)\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        def crf_loss(y_true, y_pred):\n",
    "            y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype)\n",
    "            log_likelihood, self.transitions = crf_log_likelihood(\n",
    "                y_pred,\n",
    "                tf.cast(K.argmax(y_true), dtype=tf.int32) if self.sparse_target else y_true,\n",
    "                self.sequence_lengths,\n",
    "                transition_params=self.transitions,\n",
    "            )\n",
    "            return tf.reduce_mean(-log_likelihood)\n",
    "        return crf_loss\n",
    "\n",
    "    @property\n",
    "    def accuracy(self):\n",
    "        def viterbi_accuracy(y_true, y_pred):\n",
    "            # -1e10 to avoid zero at sum(mask)\n",
    "            mask = K.cast(\n",
    "                K.all(K.greater(y_pred, -1e10), axis=2), K.floatx())\n",
    "            shape = tf.shape(y_pred)\n",
    "            sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n",
    "            y_pred, _ = crf_decode(y_pred, self.transitions, sequence_lengths)\n",
    "            if self.sparse_target:\n",
    "                y_true = K.argmax(y_true, 2)\n",
    "            y_pred = K.cast(y_pred, 'int32')\n",
    "            y_true = K.cast(y_true, 'int32')\n",
    "            corrects = K.cast(K.equal(y_true, y_pred), K.floatx())\n",
    "            return K.sum(corrects * mask) / K.sum(mask)\n",
    "        return viterbi_accuracy\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        tf.TensorShape(input_shape).assert_has_rank(3)\n",
    "        return input_shape[:2] + (self.output_dim,)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'sparse_target': self.sparse_target,\n",
    "            'supports_masking': self.supports_masking,\n",
    "            'transitions': K.eval(self.transitions)\n",
    "        }\n",
    "        base_config = super(CRF, self).get_config()\n",
    "        return dict(base_config, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f0da4916",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 308)]             0         \n",
      "                                                                 \n",
      " embedding_8 (Embedding)     (None, 308, 20)           103620    \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 308, 100)         28400     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 308, 4)           404       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 308, 4)            16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 132,440\n",
      "Trainable params: 132,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = bilstm_crf(maxlen=max_len, n_tags=n_tags, embedding_dim=20, n_words=n_words, mask_zero=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ab61be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(optimizer = Adam(learning_rate = 0.01), loss = model.layers[-1].loss, metrics = model.layers[-1].accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d58f72e",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "295561e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2444/2444 [==============================] - 1483s 604ms/step - loss: 2.4383 - viterbi_accuracy: 0.9810 - val_loss: 67.4596 - val_viterbi_accuracy: 0.8943\n",
      "{'loss': [2.4382832050323486], 'viterbi_accuracy': [0.9809593558311462], 'val_loss': [67.45957946777344], 'val_viterbi_accuracy': [0.8942558169364929]}\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_x, train_y, batch_size=32, epochs=1, verbose=1, validation_split=0.1)\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f90a51",
   "metadata": {},
   "source": [
    "### IDCNN-CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bdf99bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 308)]             0         \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, 308, 20)           103620    \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 308, 64)           3904      \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 308, 128)          24704     \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 308, 128)          49280     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 308, 4)            516       \n",
      "                                                                 \n",
      " crf_layer (CRF)             (None, 308, 4)            16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,040\n",
      "Trainable params: 182,040\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def bilstm_crf(maxlen, n_tags, embedding_dim, n_words, mask_zero, training = True):\n",
    "    \"\"\"\n",
    "    bilstm_crf - module to build BiLSTM-CRF model\n",
    "    Inputs:\n",
    "        - input_shape : tuple\n",
    "            Tensor shape of inputs, excluding batch size\n",
    "    Outputs:\n",
    "        - output : tensorflow.keras.outputs.output\n",
    "            BiLSTM-CRF output\n",
    "    \"\"\"\n",
    "    input = Input(shape = (maxlen,))\n",
    "    \n",
    "    # Embedding layer\n",
    "    embeddings = embedding_layer(input_dim=n_words, output_dim=embedding_dim, input_length=maxlen, mask_zero=mask_zero)\n",
    "    output = embeddings(input)\n",
    "\n",
    "    # BiLSTM layer\n",
    "    output = Conv1D(filters=64,\n",
    "                    kernel_size=3,\n",
    "                    activation='relu',\n",
    "                    padding='same',\n",
    "                    dilation_rate=1)(output)\n",
    "    \n",
    "    output = Conv1D(filters=128,\n",
    "                    kernel_size=3,\n",
    "                    activation='relu',\n",
    "                    padding='same',\n",
    "                    dilation_rate=1)(output)\n",
    "    \n",
    "    output = Conv1D(filters=128,\n",
    "                    kernel_size=3,\n",
    "                    activation='relu',\n",
    "                    padding='same',\n",
    "                    dilation_rate=2)(output)\n",
    "    \n",
    "    \n",
    "    # Dense layer\n",
    "    output = Dense(n_tags)(output)\n",
    "    \n",
    "    output = CRF(n_tags, name='crf_layer')(output)\n",
    "    return Model(input, output)\n",
    "\n",
    "model = bilstm_crf(maxlen=max_len, n_tags=n_tags, embedding_dim=20, n_words=n_words, mask_zero=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb443d47",
   "metadata": {},
   "source": [
    "## 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ae33d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 3s 158ms/step - loss: 941.2169 - viterbi_accuracy: 0.5467\n",
      "[941.2169189453125, 0.5467380881309509]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_x, test_y, batch_size=256)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b3845282",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = model.predict(test_x)\n",
    "test_predict = [[np.argmax(char) for char in sample] for sample in test_predict]  # get the max label_id\n",
    "test_predict_tag = [[chunk_tags[i] for i in sample ]for sample in test_predict]   # get the label of predic\n",
    "test_gold = [[np.argmax(char) for char in sample] for sample in test_y]  # get the label_id\n",
    "test_gold_tag = [[chunk_tags[i] for i in sample] for sample in test_gold]  # get the label of real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "286c14fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "扬帆远东  做  与  中国  合作  的  先行\n",
      "希腊  的  经济  结构  较  特殊  。\n",
      "海运业  雄踞  全球  之  首  ，  按  吨位  计占  世界  总数  的  １７％  。\n",
      "另外  旅游  、  侨汇  也是  经济  收入  的  重要  组成部分  ，  制造业  规  模  相  对  较小  。\n",
      "多年  来  ，  中希贸易  始终  处于  较低  的  水平  ，  希腊  几乎  没有  在  中国  投资  。\n"
     ]
    }
   ],
   "source": [
    "test_result = []\n",
    "vocab2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "_, test_content, _, _ = read_file(test_set)\n",
    "for i in range(len(test_predict)):\n",
    "# for i in range(1):\n",
    "    sentence = ''\n",
    "    s_len = len(test_content[i])\n",
    "    sample = test_predict_tag[i]\n",
    "    for j in range(s_len):\n",
    "        idx = len(sample)- s_len + j\n",
    "        if sample[idx]=='B' or sample[idx]=='M' or j==s_len-1:\n",
    "            sentence = sentence + test_content[i][j]\n",
    "        else:\n",
    "            sentence = sentence + test_content[i][j]\n",
    "            sentence = sentence + '  '\n",
    "    test_result.append(sentence)\n",
    "print('\\n'.join(test_result[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b7acdb8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "扬帆  远东  做  与  中国  合作  的  先  行\n",
      "希腊  的  经济  结构  较  特殊  。\n",
      "海运业  雄踞  全球  之  首  ，  按  吨  位计  占  世界  总数  的  １７％  。\n",
      "另外  旅游  、  侨汇  也是  经济  收入  的  重要  组成部分  ，  制造业  规模  相对  较小  。\n",
      "多年来  ，  中希  贸易  始终  处于  较低  的  水平  ，  希腊  几乎  没有  在  中国  投资  。\n"
     ]
    }
   ],
   "source": [
    "test_result = []\n",
    "vocab2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "_, test_content, _, _ = read_file(test_set)\n",
    "for i in range(len(test_predict)):\n",
    "# for i in range(1):\n",
    "    sentence = ''\n",
    "    s_len = len(test_content[i])\n",
    "    sample = test_predict_tag[i]\n",
    "    for j in range(s_len):\n",
    "        idx = len(sample)- s_len + j\n",
    "        if sample[idx]=='B' or sample[idx]=='M' or j==s_len-1:\n",
    "            sentence = sentence + test_content[i][j]\n",
    "        else:\n",
    "            sentence = sentence + test_content[i][j]\n",
    "            sentence = sentence + '  '\n",
    "    test_result.append(sentence)\n",
    "print('\\n'.join(test_result[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad4c5b",
   "metadata": {},
   "source": [
    "## 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5870404a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-09 16:50:59.131867: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bilstm_crf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f07340fa550> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f07340fa850> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.models.save_model(model, filepath = 'bilstm_crf', save_format = 'tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1eb42ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/huanfu/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/huanfu/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpupy68hk7/assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "with open('bilstm_crf.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b4a03d4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总用量 569M\r\n",
      "-rw-rw-r-- 1 huanfu huanfu 713K 2月   7 14:55 期中作业.pdf\r\n",
      "-rw-rw-r-- 1 huanfu huanfu  25K 2月   7 14:55 中文分词实验报告.md\r\n",
      "drwxr-xr-x 4 huanfu huanfu 4.0K 2月   8 16:56 bilstm_crf\r\n",
      "-rw-rw-r-- 1 huanfu huanfu 695K 2月   8 17:59 bilstm_crf.tflite\r\n",
      "drwxrwxr-x 3 huanfu huanfu 4.0K 2月   7 15:57 code\r\n",
      "drwxrwxr-x 2 huanfu huanfu 4.0K 2月   7 14:55 data\r\n",
      "-rw-rw-r-- 1 huanfu huanfu  32K 2月   8 15:13 main.ipynb\r\n",
      "-rw-rw-r-- 1 huanfu huanfu  11K 2月   7 16:02 main.py\r\n",
      "-rw-rw-r-- 1 huanfu huanfu 7.2M 2月   7 18:27 model.h5\r\n",
      "-rw-rw-r-- 1 huanfu huanfu 2.0K 2月   7 14:55 README.md\r\n",
      "-rw-rw-r-- 1 huanfu huanfu 1.5M 2月   7 14:55 report.pdf\r\n",
      "-rwxr----- 1 huanfu huanfu 559M 2月   7 17:04 sgns.context.word-character.char1-1.bz2\r\n",
      "-rw-rw-r-- 1 huanfu huanfu  24K 2月   8 18:00 wordseg.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5dd8db",
   "metadata": {},
   "source": [
    "## 词汇表和tag打包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c61096fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocabs.txt\", \"w\") as f:\n",
    "    for key, val in vocab2idx.items():\n",
    "        f.write(\"%s %d\\n\" % (key, val))\n",
    "        \n",
    "with open(\"tags.txt\", \"w\") as f:\n",
    "    for tag in chunk_tags:\n",
    "        f.write(\"%s\\n\" % tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5632ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huanfu/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_lite_support/metadata/python/metadata.py:359: UserWarning: File, 'tags.txt', does not exist in the metadata. But packing it to tflite model is still allowed.\n",
      "  \"tflite model is still allowed.\".format(f))\n",
      "/home/huanfu/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_lite_support/metadata/python/metadata.py:359: UserWarning: File, 'vocabs.txt', does not exist in the metadata. But packing it to tflite model is still allowed.\n",
      "  \"tflite model is still allowed.\".format(f))\n"
     ]
    }
   ],
   "source": [
    "from tflite_support import metadata as _metadata\n",
    "\n",
    "populator = _metadata.MetadataPopulator.with_model_file(\"bilstm_crf.tflite\")\n",
    "populator.load_associated_files([\"vocabs.txt\",\"tags.txt\"])\n",
    "\n",
    "populator.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "094e481a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "期中作业.pdf\t     data\t report.pdf\r\n",
      "中文分词实验报告.md  main.ipynb  sgns.context.word-character.char1-1.bz2\r\n",
      "bilstm_crf\t     main.py\t tags.txt\r\n",
      "bilstm_crf.tflite    model.h5\t vocabs.txt\r\n",
      "code\t\t     README.md\t wordseg.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653874b5",
   "metadata": {},
   "source": [
    "## 测试tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "421e9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"bilstm_crf.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b2807c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'input_1', 'index': 0, 'shape': array([  1, 308], dtype=int32), 'shape_signature': array([ -1, 308], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "\n",
      "[{'name': 'Identity', 'index': 204, 'shape': array([  1, 308,   4], dtype=int32), 'shape_signature': array([  1, 308,   4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(input_details)\n",
    "print(\"\")\n",
    "print(output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d178bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e6f5f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 308)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = test_x[:1].astype(np.float32)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "467a09d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 308, 4)\n"
     ]
    }
   ],
   "source": [
    "index = input_details[0]['index']\n",
    "interpreter.set_tensor(index, input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33628e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "希腊  的  经济  结构  较  特殊  。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
